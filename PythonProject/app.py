import streamlit as st
import chromadb
from pypdf import PdfReader
from openai import OpenAI
import os
import json

# =================é…ç½®åŒºåŸŸ=================
# âš ï¸ æ³¨æ„ï¼šå®é™…éƒ¨ç½²æ—¶å°½é‡ä¸è¦æŠŠ Key ç›´æ¥å†™åœ¨ä»£ç é‡Œï¼Œå»ºè®®ç”¨ç¯å¢ƒå˜é‡
API_KEY = "sk-a821aba999ce40cc9c8349cf7ac2871d"
PDF_PATH = "data.pdf"
DB_PATH = "./my_vector_db"
HISTORY_FILE = "chat_history.json"
# =========================================

# è®¾ç½®ç½‘é¡µæ ‡é¢˜
st.set_page_config(page_title="æˆ‘çš„ RAG åŠ©æ‰‹", layout="centered")
st.title("ğŸ¤– æ™ºèƒ½ RAG é—®ç­”åŠ©æ‰‹")


# --- 1. åˆå§‹åŒ–èµ„æº (ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡æ“ä½œéƒ½é‡è¯» PDF) ---
@st.cache_resource
def get_vector_db():
    print("æ­£åœ¨è¿æ¥çŸ¥è¯†åº“...")
    chroma_client = chromadb.PersistentClient(path=DB_PATH)
    collection = chroma_client.get_or_create_collection(name="my_knowledge")

    # åªæœ‰å½“æ•°æ®åº“ä¸ºç©ºæ—¶ï¼Œæ‰è¯»å– PDF
    if collection.count() == 0:
        if os.path.exists(PDF_PATH):
            print("é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨è¯»å– PDF å¹¶æ„å»ºç´¢å¼•...")
            reader = PdfReader(PDF_PATH)
            pdf_text = ""
            for page in reader.pages:
                pdf_text += page.extract_text()

            # åˆ‡åˆ†æ–‡æœ¬
            chunks = [pdf_text[i:i + 300] for i in range(0, len(pdf_text), 300)]
            collection.add(documents=chunks, ids=[str(i) for i in range(len(chunks))])
            print("ç´¢å¼•æ„å»ºå®Œæˆï¼")
        else:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ° PDF æ–‡ä»¶ã€‚")
    return collection


# åŠ è½½æ•°æ®åº“ (è¿™ä¸€æ­¥åªä¼šåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ¯”è¾ƒæ…¢)
collection = get_vector_db()
client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com")

# --- 2. ç®¡ç†èŠå¤©è®°å½• (Session State) ---
# åˆå§‹åŒ– session_stateï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
if "messages" not in st.session_state:
    # å°è¯•ä»æœ¬åœ°åŠ è½½æ—§è®°å½•
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            st.session_state.messages = json.load(f)
    else:
        st.session_state.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·ç»“åˆä¸Šä¸‹æ–‡å’Œå‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚"}
        ]

# --- 3. æ¸²æŸ“èŠå¤©ç•Œé¢ ---
# éå†å†å²è®°å½•å¹¶åœ¨ç½‘é¡µä¸Šæ˜¾ç¤º (è·³è¿‡ system æ¶ˆæ¯)
for msg in st.session_state.messages:
    if msg["role"] != "system":
        # å¦‚æœæ˜¯ user å‘çš„æ¶ˆæ¯ï¼Œä½†åŒ…å«äº†ã€èµ„æ–™ã€‘å‰ç¼€ï¼Œæˆ‘ä»¬åœ¨ç•Œé¢ä¸Šåªæ˜¾ç¤ºã€é—®é¢˜ã€‘éƒ¨åˆ†ï¼Œæ¯”è¾ƒç¾è§‚
        content_to_show = msg["content"]
        if msg["role"] == "user" and "ã€èµ„æ–™ã€‘" in content_to_show:
            # ç®€å•çš„åˆ†å‰²é€»è¾‘ï¼Œåªæ˜¾ç¤ºâ€œã€é—®é¢˜ã€‘â€åé¢çš„å†…å®¹
            try:
                content_to_show = content_to_show.split("ã€é—®é¢˜ã€‘")[1]
            except:
                pass  # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œå°±æ˜¾ç¤ºåŸæ–‡

        with st.chat_message(msg["role"]):
            st.markdown(content_to_show)

# --- 4. å¤„ç†ç”¨æˆ·è¾“å…¥ ---
# st.chat_input ç›¸å½“äºåŸæ¥çš„ input()
if user_query := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):

    # 1. åœ¨ç•Œé¢ä¸Šç«‹å³æ˜¾ç¤ºç”¨æˆ·çš„é—®é¢˜
    with st.chat_message("user"):
        st.markdown(user_query)

    # 2. RAG æ£€ç´¢é€»è¾‘
    results = collection.query(query_texts=[user_query], n_results=2)
    if results['documents'] and results['documents'][0]:
        retrieved_text = " ".join(results['documents'][0])
    else:
        retrieved_text = "æ— ç›¸å…³èµ„æ–™"

    # 3. æ„é€ å‘ç»™ AI çš„å®Œæ•´ Prompt
    full_prompt = f"ã€èµ„æ–™ã€‘{retrieved_text}\nã€é—®é¢˜ã€‘{user_query}"

    # å°†å®Œæ•´ Prompt åŠ å…¥å†å²è®°å½• (ä¸ºäº†è®© AI è®°ä½ä¸Šä¸‹æ–‡)
    st.session_state.messages.append({"role": "user", "content": full_prompt})

    # 4. è°ƒç”¨ AI å¹¶æµå¼è¾“å‡ºç»“æœ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # ä¸ºäº†ä½“éªŒæ›´å¥½ï¼Œè¿™é‡ŒæŠŠ stream æ”¹æˆäº† True (å¯é€‰)
        stream = client.chat.completions.create(
            model="deepseek-chat",
            messages=st.session_state.messages,
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "â–Œ")  # åŠ ä¸ªå…‰æ ‡ç‰¹æ•ˆ

        message_placeholder.markdown(full_response)

    # 5. ä¿å­˜ AI å›å¤
    st.session_state.messages.append({"role": "assistant", "content": full_response})

    # 6. æŒä¹…åŒ–ä¿å­˜åˆ° JSON (æ¯èŠä¸€æ¬¡å­˜ä¸€æ¬¡)
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)